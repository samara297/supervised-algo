# -*- coding: utf-8 -*-
"""modellinear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcpoVRFhS4Z9tuIhgXk5eWxWN4v063eB
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np

df = pd.read_csv('/content/drive/MyDrive/all ml/Student_Performance.csv')

df.shape

df.info()

# Encode categorical feature
df["Extracurricular Activities"] = df["Extracurricular Activities"].map({"Yes": 1, "No": 0})

# Define features and target variable
X = df.drop(columns=["Performance Index"])
y = df["Performance Index"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hyperparameter tuning using Ridge regression
param_grid = {"alpha": [0.01, 0.1, 1, 10, 100]}
ridge = Ridge()
grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring="neg_mean_squared_error")
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test)

# Calculate MAE and MSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(mae)
print(mse)

# Plot distribution of actual vs predicted values
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test, label="Actual", shade=True, color="blue")
sns.kdeplot(y_pred, label="Predicted", shade=True, color="red")
plt.legend()
plt.title("Distribution of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")
plt.show()

# Check data types of y_test and y_pred
y_test.dtype, y_pred.dtype

# Check for NaN or infinite values in y_test and y_pred
# y_test.isna().sum(), y_pred.isnan().sum(), (y_pred == float("inf")).sum(), (y_pred == float("-inf")).sum()
# import numpy as np

# Check for NaN or infinite values in y_test and y_pred
nan_y_test = np.isnan(y_test).sum()
nan_y_pred = np.isnan(y_pred).sum()
inf_y_pred = np.isinf(y_pred).sum()

nan_y_test, nan_y_pred, inf_y_pred

# Convert y_test and y_pred to Pandas Series for compatibility
y_test_series = pd.Series(y_test, name="Actual")
y_pred_series = pd.Series(y_pred, name="Predicted")

# Re-attempting the distribution plot
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test_series, label="Actual", color="blue", fill=True)
sns.kdeplot(y_pred_series, label="Predicted", color="red", fill=True)
plt.legend()
plt.title("Distribution of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")
plt.show()

# Check unique values and basic statistics of y_pred_series
y_pred_series.describe(), y_pred_series.unique()[:10]

# Alternative visualization using histogram
plt.figure(figsize=(8, 5))
plt.hist(y_test, bins=30, alpha=0.5, label="Actual", color="blue", density=True)
plt.hist(y_pred, bins=30, alpha=0.5, label="Predicted", color="red", density=True)
plt.legend()
plt.title("Histogram of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")
plt.show()

# Create a DataFrame with actual and predicted values
results_df = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})

# Add a column for absolute difference
results_df["Absolute Difference"] = abs(results_df["Actual"] - results_df["Predicted"])

# Display the first few rows
results_df

results_df['Absolute Difference'].describe()

results_df['Absolute Difference'].median()

"""# knn regression"""

from sklearn.neighbors import KNeighborsRegressor

# Initialize and train the KNN regressor (without hyperparameter tuning)
knn = KNeighborsRegressor(n_neighbors=5) # You can change the n_neighbors value
knn.fit(X_train, y_train)

# Make predictions
y_pred = knn.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid for KNN
param_grid_knn = {
    'n_neighbors': range(1, 31),  # Test a range of neighbors
    'weights': ['uniform', 'distance'],  # Consider different weighting schemes
    'metric': ['euclidean', 'manhattan'] # Explore different distance metrics
}

# Perform GridSearchCV for KNN
knn_grid_search = GridSearchCV(KNeighborsRegressor(), param_grid_knn, cv=5, scoring='neg_mean_squared_error')
knn_grid_search.fit(X_train, y_train)

print("Best hyperparameters (GridSearchCV):", knn_grid_search.best_params_)
print("Best score (GridSearchCV):", knn_grid_search.best_score_)

best_knn_grid = knn_grid_search.best_estimator_
y_pred_knn_grid = best_knn_grid.predict(X_test)
mae_knn_grid = mean_absolute_error(y_test, y_pred_knn_grid)
mse_knn_grid = mean_squared_error(y_test, y_pred_knn_grid)
print(f"KNN Grid MAE: {mae_knn_grid}")
print(f"KNN Grid MSE: {mse_knn_grid}")

# # Perform RandomizedSearchCV for KNN
# knn_random_search = RandomizedSearchCV(KNeighborsRegressor(), param_grid_knn, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)
# knn_random_search.fit(X_train, y_train)

# print("\nBest hyperparameters (RandomizedSearchCV):", knn_random_search.best_params_)
# print("Best score (RandomizedSearchCV):", knn_random_search.best_score_)

# best_knn_random = knn_random_search.best_estimator_
# y_pred_knn_random = best_knn_random.predict(X_test)
# mae_knn_random = mean_absolute_error(y_test, y_pred_knn_random)
# mse_knn_random = mean_squared_error(y_test, y_pred_knn_random)
# print(f"KNN Random MAE: {mae_knn_random}")
# print(f"KNN Random MSE: {mse_knn_random}")

# Plot distribution of actual vs predicted values
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test, label="Actual", shade=True, color="red")
sns.kdeplot(y_pred, label="Predicted", shade=True, color="red")
plt.legend()
plt.title("Distribution of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")

# Create a DataFrame with actual and predicted values
results_df = pd.DataFrame({"Actual": y_test, "Predicted": y_pred_knn_grid})

# Add a column for absolute difference
results_df["Absolute Difference"] = abs(results_df["Actual"] - results_df["Predicted"])

# # Display the first few rows
# results_df

results_df['Absolute Difference'].describe()

col=['Model' , 'MAE','MSE']
knn = pd.DataFrame([['knn',mae_knn_grid,mse_knn_grid]],columns=col)

result=pd.DataFrame()
result=pd.concat([result,knn],ignore_index=True)
result

"""# linear"""

# Cell 71: Change the variable name from 'knn' to something else like 'linear_model'
from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

y_pred = linear_model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")

# # Cell 75: Keep the DataFrame 'knn' as it is
# result=pd.DataFrame()
# result=pd.concat([result,knn,linear],ignore_index=True)
# result

col = ['Model', 'MAE', 'MSE']
linear_results = pd.DataFrame([['linear', mae, mse]], columns=col)

result=pd.DataFrame()
result=pd.concat([result,knn,linear_results],ignore_index=True)
result

"""# decisiontree"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings("ignore")

dept = [1, 5, 10, 50, 100, 500, 1000]
min_samples =  [30, 40, 60, 80,100,120]


param_grid={'min_samples_split':min_samples , 'max_depth':dept}
clf = DecisionTreeRegressor()

model = GridSearchCV(clf,param_grid,scoring='neg_mean_squared_error',n_jobs=-1,cv=3)
model.fit(X_train, y_train)


# best_dt_grid = model.best_estimator_
# y_pred_dt_grid = model.predict(X_test)
# y_train_dt_grid=model.predict(X_train)

print("optimal min_samples_split",model.best_estimator_.min_samples_split)
print("optimal max_depth",model.best_estimator_.max_depth)

# dt1= mean_absolute_error(y_test, y_pred)
# dt2= mean_squared_error(y_test, y_pred)

# print(f"Mean Absolute Error: {dt1}")
# print(f"Mean Squared Error: {dt2}")

clf = DecisionTreeRegressor(max_depth = 30,min_samples_split = 50)
clf.fit(X_train,y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)


dt1= mean_absolute_error(y_test, y_pred)
dt2= mean_squared_error(y_test, y_pred)

print(f"Mean Absolute Error: {dt1}")
print(f"Mean Squared Error: {dt2}")

# Plot distribution of actual vs predicted values
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test, label="Actual", shade=True, color="green")
sns.kdeplot(y_pred, label="Predicted", shade=True, color="red")
plt.legend()
plt.title("Distribution of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")

col = ['Model', 'MAE', 'MSE']
decisiontree = pd.DataFrame([['decisiontree', mae, mse]], columns=col)





# a=pd.DataFrame({
#     "Model":["KNN","linear","decisiontree"],
#     "mse":[1234,123],
#     "mae":[1.835467,5.363373]
# })

result=pd.DataFrame()
result=pd.concat([result,knn,linear_results,decisiontree],ignore_index=True)
result

"""# random forest"""

from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_curve

dept = [1, 5, 10, 50, 100, 500, 1000]
n_estimators =  [10, 40, 60, 80, 100, 120]

param_grid={'n_estimators':n_estimators , 'max_depth':dept}
clf = RandomForestRegressor()
model = GridSearchCV(clf,param_grid,scoring='neg_mean_squared_error',n_jobs=-1,cv=3)
model.fit(X_train, y_train)
print("optimal n_estimators",model.best_estimator_.n_estimators)
print("optimal max_depth",model.best_estimator_.max_depth)

best_min_samples_split = model.best_estimator_.n_estimators
best_max_depth = model.best_estimator_.max_depth
clf = RandomForestRegressor(max_depth = best_max_depth,min_samples_split =best_min_samples_split )
clf.fit(X_train,y_train)
# Predict on the test set and cross-validation set
y_train_pred=clf.predict(X_train)
y_test_pred = clf.predict(X_test)


dt1= mean_absolute_error(y_test, y_pred)
dt2= mean_squared_error(y_test, y_pred)

print(f"Mean Absolute Error: {dt1}")
print(f"Mean Squared Error: {dt2}")

# Plot distribution of actual vs predicted values
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test, label="Actual", shade=True, color="red")
sns.kdeplot(y_pred, label="Predicted", shade=True, color="green")
plt.legend()
plt.title("Distribution of Actual vs Predicted Performance Index")
plt.xlabel("Performance Index")
plt.ylabel("Density")

col = ['Model', 'MAE', 'MSE']
randomforest= pd.DataFrame([['randomforest', mae, mse]], columns=col)

result=pd.DataFrame()
result=pd.concat([result,knn,linear_results,decisiontree,randomforest],ignore_index=True)
result

